{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wachacha-jay/Computer_vision_Image_classification/blob/main/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ6VGuymgdoR",
        "outputId": "f8f987b7-3d69-4514-bb9d-1c13373f435e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8clnYhpZdl3n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchinfo import summary\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRbdfUHwdoxG"
      },
      "outputs": [],
      "source": [
        "print(\"Platform:\", sys.platform)\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"---\")\n",
        "print(\"matplotlib version:\", matplotlib.__version__)\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"PIL version:\", PIL.__version__)\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"torchvision version:\", torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgcGOKOkdwZ7",
        "outputId": "8aa3ab14-742d-473f-a2e4-861b8e167b7d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device.\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eS3UPNUYhMN1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "LfhsoIR82px_",
        "outputId": "4352b33d-b5f4-46b8-9083-edaf8f71edc9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.6.15)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-21119959-eff7-4293-a577-83fd9532247d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-21119959-eff7-4293-a577-83fd9532247d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install kagglehub\n",
        "\n",
        "# Upload your Kaggle API key (kaggle.json)\n",
        "from google.colab import files\n",
        "files.upload()  # Select your kaggle.json file\n",
        "\n",
        "# Move the key to the correct directory\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avFVTuHT-BPD"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"srg9000/cassava-plant-disease-merged-20192020\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)  # This will show where the dataset is stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGJjF680-9II"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Copy the dataset to a new location\n",
        "destination = \"./cassava_dataset\"\n",
        "if not os.path.exists(destination):\n",
        "    shutil.copytree(path, destination)\n",
        "    print(f\"Dataset copied to {destination}\")\n",
        "else:\n",
        "    print(f\"{destination} already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNn5MztuBb1s"
      },
      "outputs": [],
      "source": [
        "destination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwGQnwUTFKOj"
      },
      "outputs": [],
      "source": [
        "os.listdir(destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlKmj5aaFX7l"
      },
      "outputs": [],
      "source": [
        "data_dir = os.path.join(destination, 'merged_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILQkxZBWFxSu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(data_dir)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHihgI6PGEB-"
      },
      "outputs": [],
      "source": [
        "labels_dict = {\n",
        "    0: 'Cassava Bacterial Blight (CBB)',\n",
        "    1: 'Cassava Brown Streak Disease (CBSD)',\n",
        "    2: 'Cassava Green Mottle (CGM)',\n",
        "    3: 'Cassava Mosaic Disease (CMD)',\n",
        "    4: 'Healthy'\n",
        "}\n",
        "\n",
        "# Add a new column 'disease_name' by mapping 'labels' to 'labels_dict'\n",
        "df['disease_name'] = df['labels'].map(labels_dict)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcXzTvL_IGUN"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGCl4425qXEK"
      },
      "outputs": [],
      "source": [
        "train_path = os.path.join(destination, \"train_images\", \"train_images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSIFDJFFrh7L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# ===== CONFIGURATION =====\n",
        "source_folder = train_path  # Folder containing all your images\n",
        "destination_root = \"train\"        # Main folder to be created\n",
        "\n",
        "\n",
        "# ===== FOLDER CREATION =====\n",
        "# Create main folder and subfolders (with existence checks)\n",
        "os.makedirs(destination_root, exist_ok=True)  # Main 'train' folder\n",
        "print(f\"✅ Created main folder: {destination_root}/\")\n",
        "\n",
        "for disease in labels_dict.values():\n",
        "    subfolder = os.path.join(destination_root, disease)\n",
        "    os.makedirs(subfolder, exist_ok=True)\n",
        "    print(f\"  ├── Created subfolder: {disease}/\")\n",
        "\n",
        "# ===== IMAGE ORGANIZATION =====\n",
        "success_count = 0\n",
        "missing_count = 0\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    src = os.path.join(source_folder, row['image_id'])\n",
        "    dest = os.path.join(destination_root, labels_dict[row['labels']], row['image_id'])\n",
        "\n",
        "    try:\n",
        "        shutil.copy2(src, dest)  # copy2 preserves metadata\n",
        "        success_count += 1\n",
        "    except FileNotFoundError:\n",
        "        print(f\"⚠️ Missing: {row['image_id']}\")\n",
        "        missing_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error copying {row['image_id']}: {str(e)}\")\n",
        "\n",
        "# ===== RESULTS SUMMARY =====\n",
        "print(\"\\n📊 Organization Complete!\")\n",
        "print(f\"✅ Successfully copied: {success_count} files\")\n",
        "print(f\"⚠️  Missing files: {missing_count}\")\n",
        "print(f\"📁 Final structure in: {os.path.abspath(destination_root)}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cggrHRqUAfFF"
      },
      "outputs": [],
      "source": [
        "classes = os.listdir(\"/content/train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUz0-bGqAwfB"
      },
      "outputs": [],
      "source": [
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "destination_root = os.path.join(\"/content/train\")"
      ],
      "metadata": {
        "id": "8yFutHQS9rxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "destination_root = Path(destination_root)"
      ],
      "metadata": {
        "id": "t49Mvs8L_T_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(destination_root)"
      ],
      "metadata": {
        "id": "rXXZt1EvZ99o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = list(destination_root.iterdir())\n",
        "train_dir[0]"
      ],
      "metadata": {
        "id": "1MVJE_Y6aNT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "healthy_dir =train_dir[0]\n",
        "sample_img_path = list(healthy_dir.iterdir())[0]\n",
        "sample_img_path"
      ],
      "metadata": {
        "id": "rcTLkip47Pn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets investigate one of the images in the healthy directory\n",
        "sample_img = PIL.Image.open(sample_img_path)\n",
        "sample_img\n",
        "print(f\"The image has a shape of: {sample_img.size}.\")\n",
        "print(f\"The image has a mode of: {sample_img.mode}.\")"
      ],
      "metadata": {
        "id": "REphENRc72pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to convert image to RGB\n",
        "class ConvertToRGB:\n",
        "    def __call__(self, img):\n",
        "        if img.mode != \"RGB\":\n",
        "            img = img.convert(\"RGB\")\n",
        "        return img"
      ],
      "metadata": {
        "id": "BjfRhMH4dfVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets build a pipeline to convert images to RGB Resize them to a good size and convert the images to tensors\n",
        "transform = transforms.Compose([\n",
        "    ConvertToRGB(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "hCJZfRGC65Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets convert our directory into a dataset with the class labels representinting classes for doing our classification\n",
        "dataset = datasets.ImageFolder(root=destination_root, transform=transform, target_transform=None)\n",
        "dataset.classes"
      ],
      "metadata": {
        "id": "29PAjD4W9UZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to investigate further how our dataset is arranged the dataset class has an attribute img which which returns path to our image and the class index\n",
        "dataset.imgs[0]\n"
      ],
      "metadata": {
        "id": "wmbZAgJj96pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we can also investigate the class indexes with the attribute class_to index which returns a dictionary of classes mapped to the index\n",
        "dataset.class_to_idx"
      ],
      "metadata": {
        "id": "EOLIhp6K-koN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classes:\")\n",
        "print(dataset.classes)\n",
        "print(f\"That's {len(dataset.classes)} classes\")\n",
        "print()\n",
        "print(\"Tensor shape for one image:\")\n",
        "print(dataset[0][0].shape)"
      ],
      "metadata": {
        "id": "asT9wk_8_08p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normally we would work with the whole dataset but to make our work easier using pytorch it is reccomended we create a dataloader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "207voeDIBw4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets investigate our data loader our dataloader using iter and next\n",
        "first_batch = next(iter(dataloader))\n",
        "print(f\"Shape of one batch: {first_batch[0].shape}\")\n",
        "print(f\"Shape of labels: {first_batch[1].shape}\")"
      ],
      "metadata": {
        "id": "GozQJkPyChmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we have our loader to train our model but first\n",
        "When we loop over this loader, it'll produce small batches of our images. This is what we want — these are the \"minibatches\" that will speed up our computations. In our case, each batch is  32  images, with each image  3  x  224  x  224 . It also provides us with the labels for the correct answers. This is the information we need to train a network. Normally we would split our data into training and validation test but to maximize our training accuracy pytorch works well with normalized data which is data with a mean of 0 and starndard deviation of 1, this is exactly what we are going to do, lets investigate our mean and starndard deviation then add them to our transform then we can split our training and validation sets"
      ],
      "metadata": {
        "id": "M0SmymbeFIrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_std(loader):\n",
        "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "    for data, _ in loader:\n",
        "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
        "        num_batches += 1\n",
        "\n",
        "    #mean calculation\n",
        "    mean = channels_sum / num_batches\n",
        "\n",
        "    #std calculation\n",
        "    std = (channels_squared_sum / num_batches - mean**2)**0.5\n",
        "\n",
        "    return mean, std\n"
      ],
      "metadata": {
        "id": "ixyqn-hyF22S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = get_mean_std(dataloader)\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard deviation: {std}\")"
      ],
      "metadata": {
        "id": "PNQvECN3KyA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now tht we have the mean and starndard deviation we can go on and normalize our data, the beast way being doing it in the transformer oipeline"
      ],
      "metadata": {
        "id": "iAfwFnviMVMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_norm = transforms.Compose(\n",
        "    [\n",
        "        ConvertToRGB(),\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "uqXKxPgVMnQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_norm = datasets.ImageFolder(root=destination_root, transform=transform_norm)"
      ],
      "metadata": {
        "id": "lOQFlrcHw8m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_dataloader = DataLoader(dataset=dataset_norm, batch_size=batch_size)\n",
        "first_batch_norm = next(iter(norm_dataloader))\n",
        "print(f\"Shape of one batch: {first_batch_norm[0].shape}\")\n",
        "print(f\"Shape of labels: {first_batch_norm[1].shape}\")"
      ],
      "metadata": {
        "id": "TBlmG0HHx0M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets look t the mean and std after our transformtion\n",
        "mean, std = get_mean_std(norm_dataloader)\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard deviation: {std}\")"
      ],
      "metadata": {
        "id": "HGQOqCER0XBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Validtion split\n",
        "Now we will split our data into training and validtion when mking the dataloader to use in training our model. This will enable us assess the perfomance of our model"
      ],
      "metadata": {
        "id": "m6hjrUI11Wkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genenerator = torch.Generator().manual_seed(42)\n",
        "train_dataset, val_dataset = random_split(dataset_norm, [0.8, 0.2], generator=genenerator)\n",
        "length_train = len(train_dataset)\n",
        "length_val = len(val_dataset)\n",
        "length_dataset = len(dataset_norm)\n",
        "percent_train = np.round(100 * length_train / length_dataset, 2)\n",
        "percent_val = np.round(100 * length_val / length_dataset, 2)\n",
        "\n",
        "print(f\"Train data is {percent_train}% of full data\")\n",
        "print(f\"Validation data is {percent_val}% of full data\")"
      ],
      "metadata": {
        "id": "nKk2Brq40NQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets check the number of categories we have in each dataset, we know we had different numbers of categories distributed"
      ],
      "metadata": {
        "id": "MSOg9G4h3Sv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def class_counts(dataset):\n",
        "    c = Counter(x[1] for x in tqdm(dataset))\n",
        "    class_to_index = dataset.dataset.class_to_idx\n",
        "    return pd.Series({cat: c[idx] for cat, idx in class_to_index.items()})\n"
      ],
      "metadata": {
        "id": "09mNxcYI3zSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets start with the training set\n",
        "train_count = class_counts(train_dataset)\n",
        "train_count"
      ],
      "metadata": {
        "id": "UQhVVJ1C43O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets plot a bar chart to investigate the distribution this will be able to explain to us if we are dealing with imbalanced dta and the implictions to our model perfomance\n",
        "train_count.plot(kind=\"bar\")\n",
        "# Add axis labels and title\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Frequency [count]\")\n",
        "plt.title(\"Class Distribution in Training Set\");"
      ],
      "metadata": {
        "id": "0RGhz47Z5GUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets start with the training set\n",
        "val_count = class_counts(val_dataset)\n",
        "val_count"
      ],
      "metadata": {
        "id": "8OPRO3Zc5q8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class distribution\n",
        "val_count.plot(kind=\"bar\")\n",
        "# Add axis labels and title\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Frequency [count]\")\n",
        "plt.title(\"Class Distribution in Validation Set\");"
      ],
      "metadata": {
        "id": "1X3edW6K52vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we are good lets build a training and validation loader, we will set shuffle True to only the training set\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, generator=genenerator)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "single_batch = next(iter(train_dataloader))[0]\n",
        "print(f\"Shape of one batch: {single_batch.shape}\")"
      ],
      "metadata": {
        "id": "JKZ_Yt-T8IYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL ARCHITECHTURE\n",
        "Things are looking good we have our training and validation datAloader with a batchsize of 32 with images of 3 color channels and size of 224 by 224 now we are ready to move to the next step of bulding our model. Since we are dealing with  clasification problem we will build layers of convolution networks connected to a fully dense layer."
      ],
      "metadata": {
        "id": "YQp6A5v09rpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WE will build our model using sequential in the pytorch library\n",
        "model_seq = torch.nn.Sequential()"
      ],
      "metadata": {
        "id": "TkgwDeXO-veE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We will create a convolution layer, pass in the number of channels in in our case 3 color channels and number of channels out, lets make it 16 a multiple of 2, use kernel_size of 3x3 and move a stride of 1\n",
        "2. Pass an activation function in this case we will use Relu\n",
        "3. then we will add a pooling lyer to reduce the output and in this case we will use maxpooling"
      ],
      "metadata": {
        "id": "D_gHPPz0FY5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1)\n",
        "model_seq.append(conv2d1)\n",
        "model_seq.append(torch.nn.ReLU())\n",
        "maxpool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "model_seq.append(maxpool1)"
      ],
      "metadata": {
        "id": "V2_GBV_YAW5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets investigate with our test batch and see what we get\n",
        "model_seq(single_batch).shape"
      ],
      "metadata": {
        "id": "WhzMoPwLL9Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will add a convolution layer taking in our  16  channels and outputting  32  channels, with a  3  x  3  kernel and padding of  1 . Follow that with a ReLU, and a max pool of size  2  x  2."
      ],
      "metadata": {
        "id": "I6esQ8mbNG73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "model_seq.append(conv2d2)\n",
        "model_seq.append(torch.nn.ReLU())\n",
        "maxpool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "model_seq.append(maxpool2)"
      ],
      "metadata": {
        "id": "xhROYmdCNRIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets investigate with our test batch and see what we get after maxpool 2\n",
        "model_seq(single_batch).shape"
      ],
      "metadata": {
        "id": "BRN_AKdWN2A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets add another layer of convolution network\n",
        "conv2d3 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "model_seq.append(conv2d3)\n",
        "model_seq.append(torch.nn.ReLU())\n",
        "maxpool3 = torch.nn.MaxPool2d(kernel_size=2)\n",
        "model_seq.append(maxpool3)"
      ],
      "metadata": {
        "id": "Of2PwGuUOEBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets investigate with our test batch and see what we get after maxpool 3\n",
        "model_seq(single_batch).shape"
      ],
      "metadata": {
        "id": "4_9bP-ZcP4j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We  are getting an out of 64 channel but for our dense layer we require a dimensional tensor which will be our batchsize and 64 x 27 x 27. For this we will flatten our otput using the flattening layer"
      ],
      "metadata": {
        "id": "yXa6KLdtQHTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq.append(torch.nn.Flatten())\n",
        "model_seq(single_batch).shape"
      ],
      "metadata": {
        "id": "DLbGmLaJRLzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point we have a flat input, and can build a normal set of dense layers. You can think of the convolution/max pool layers as having done the image processing. Now we need to do the actual classification. It turns out that dense layers are good at that task.\n",
        "\n",
        "We could add a single layer and just go straight to our output $5$ classes. But we'll get better performance by adding a few dense layers, `Linear` in PyTorch's terminology, first. For these layers, we need to tell it the size of the input, and how many neurons we want in the layer. Since the input is our previous layer, we tell it that size. We'll add a layer of $500$ neurons."
      ],
      "metadata": {
        "id": "ggI980abSPF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# But first lets add a Dropout layer\n",
        "model_seq.append(torch.nn.Dropout(p=0.5))"
      ],
      "metadata": {
        "id": "xU566hR2STIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Then we add  linear layer nd an activation function\n",
        "model_seq.append(torch.nn.Linear(in_features=64*27*27, out_features=500))\n",
        "model_seq.append(torch.nn.ReLU())"
      ],
      "metadata": {
        "id": "xEC__WLZSuqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets ensure we get our 500 AS OUR OUTPUT\n",
        "model_seq(single_batch).shape"
      ],
      "metadata": {
        "id": "Y_ZzS__4TodO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will add an output layer to get our five classes but first lets add  dropout lyer then a linear output lyer with 5 neurons\n",
        "model_seq.append(torch.nn.Dropout(p=0.5))\n",
        "model_seq.append(torch.nn.Linear(in_features=500, out_features=5))\n",
        "model_seq(single_batch).shape"
      ],
      "metadata": {
        "id": "78pmJDxoUatZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start training, let's put all the model code in one place. This is how you'd do it in practice, to prevent errors."
      ],
      "metadata": {
        "id": "zC25p8VQVbhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d(kernel_size=2),\n",
        "    torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d(kernel_size=2),\n",
        "    torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d(kernel_size=2),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Dropout(p=0.5),\n",
        "    torch.nn.Linear(in_features=64*27*27, out_features=500),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(p=0.5),\n",
        "    torch.nn.Linear(in_features=500, out_features=5)\n",
        ")"
      ],
      "metadata": {
        "id": "4xGOy1XrViJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To mke sure ou model out put is correct"
      ],
      "metadata": {
        "id": "SA1T1qxCW-En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "height, width = 224, 224\n",
        "summary(model, input_size=(batch_size, 3, height, width))"
      ],
      "metadata": {
        "id": "sz1ThWaCXGfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL TRAINING\n",
        "\n",
        "Now that we have finished building our model it time for trining according to our summry our model has over 23 million parameters. This will tke a long time to train, but first lets define the loss function and the optimizer we will be using. Since its a classification problem we will use Cross entropy and Adam optimizer"
      ],
      "metadata": {
        "id": "w10uNcgtXbAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "mxEd9LpxYPcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets crete our training function we will use a for loop to iterate through all the batches and epochs we will also be evaluating our model perfomance by calculating training and validation loss"
      ],
      "metadata": {
        "id": "ICEOHFUNZEo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(model, train_dataloader, val_dataloader, loss_fn, optimizer, epochs, device):\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      y_pred = model(X)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch, (X, y) in enumerate(val_dataloader):\n",
        "        model.eval()\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_pred = model(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        val_loss += loss.item()\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        val_total += y.size(0)\n",
        "        val_correct += (predicted == y).sum().item()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "  return train_loss, val_loss\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "i0hNh71ZYqIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now that this is complete we will train our model for 8 epochs"
      ],
      "metadata": {
        "id": "4FP15vHVcC94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer(model=model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, loss_fn=loss_fn, optimizer=optimizer, epochs=8, device=device)\n",
        ""
      ],
      "metadata": {
        "id": "lxFZ4ortcSaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rYgQra8H_ZS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Judging by the above figures the model is performing poorly and this may be attributed to by various factures.\n",
        "1. imbalanced data\n",
        "2. Our model might be to shallow\n",
        "So in the next trial we will try to undersample our dataset and also use a pretrained model from pytorch and see if our perfomance will improve but in the meanwhile lets sve our model."
      ],
      "metadata": {
        "id": "nXQrBKgi_hev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"trained_model.pth\")"
      ],
      "metadata": {
        "id": "MGt0mU5qAYOS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2CiFJpHgaWw/fy9UyZtzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}